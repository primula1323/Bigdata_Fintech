{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Simple Sentiment Analysis\n",
        "\n",
        "In this series we'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "In this first notebook, we'll start very simple to understand the general concepts whilst not really caring about good results. Further notebooks will build on this knowledge and we'll actually get good results.\n",
        "\n",
        "### Introduction\n",
        "\n",
        "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$.\n",
        "\n",
        "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
        "\n",
        "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$, (also known as a fully connected layer), to receive our predicted sentiment, $\\hat{y} = f(h_T)$.\n",
        "\n",
        "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
        "\n",
        "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
      ],
      "metadata": {
        "id": "SDtiiCVKB92o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGavi0YCMMHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006443cc-57de-4ba2-e617-85ff4438bb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "import portalocker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access to the raw dataset iterators\n",
        "\n",
        "The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the AG_NEWS dataset iterators yield the raw data as a tuple of label and text.\n",
        "\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fhs5A0NW6QBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "uVG0OVuf0q_v",
        "outputId": "9aaafebb-bba2-4075-b9b9-cf2e0d9edb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.17.1+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "import torchtext\n",
        "import torch\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Cxk8oEtQMQ8d",
        "outputId": "938ff18d-b41a-4e58-d6cc-58aba0ae28ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.17.1+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.scaler.com/topics/pytorch/map-style-vs-itetrable-datasets/\n",
        "\n",
        "# Map-style Dataset\n",
        "\n",
        "A map-style dataset is the one that implements the __getitem__() and __len__() protocols and can be represented as a map from (possibly non-integral) indices or keys to the data samples.\n",
        "\n",
        "For example, when accessed with dataset[index], such a dataset could read the index-th element and its corresponding label from the data source, which could be a folder located on the disk.\n",
        "\n",
        "The torch.utils.data.Dataset class is the base class providing support for Map-style datasets.\n",
        "\n",
        "The len method simply returns the length of the dataset. While it is not strictly an abstract method, it is recommended that every subclass of the Dataset class implements this. The reason why that is so is that this method is expected to return the size of the dataset by many implementations of the class torch.utils.data.Sampler and the default options of the classtorch.utils.data.DataLoader.\n",
        "\n",
        "__getitem__ is an abstract method in the abstract Dataset class and hence any subclass of torch.utils.data.The dataset class should implement this method.\n",
        "\n",
        "This is a dunder method and deals with returning the input features and the corresponding label of a particular data point at a specified index."
      ],
      "metadata": {
        "id": "rDXWiRNm_a0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MapDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data['sentence'][index]\n",
        "\n"
      ],
      "metadata": {
        "id": "ThituKDROZZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterable Style Dataset\n",
        "\n",
        "Iterable style datasets are primarily useful when the data arrives as part of a stream and when each data point cannot be mapped into an index.\n",
        "\n",
        "An iterable-style dataset can be created as an instance of a subclass of the class IterableDataset that implements the __iter__() protocol and represents an iterable over the data samples. This dataset type is particularly suitable for cases where random data reads are costly or even improbable and where the batch size depends on the fetched data rather than being pre-defined/configurable using a dataloader.\n",
        "\n",
        "For example, when called iter on like iter(dataset), such a dataset would return a stream of data reading from a database, from website calls over a network or a remote server, or even from logs generated in real-time.\n",
        "\n",
        "Unlike map-style datasets where the data loading order can be specified via the dataloader, the data loading order for iterable-style datasets is entirely controlled by the user-defined iterable. This also means that iterable size datasets easily allow for dynamic batch sizes as iter protocol makes it possible to yield a batch of data at a time rather than a single data point.\n",
        "\n",
        "torch.utils.data.IterableDataset class is the PyTorch primitive concerned with supporting iterable style datasets."
      ],
      "metadata": {
        "id": "A39eeayc_zqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "import math\n",
        "\n",
        "# custom iterable dataset class\n",
        "class MyIterableDataset(IterableDataset):\n",
        "    def __init__(self, start, end):\n",
        "        super(MyIterableDataset).__init__()\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = torch.utils.data.get_worker_info() # when called in a worker process, returns information about the worker\n",
        "        # Single-process data loading\n",
        "        if worker_info is None:\n",
        "            iter_start = self.start\n",
        "            iter_end = self.end\n",
        "\n",
        "        # multi-process data loading\n",
        "        else:\n",
        "            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
        "            worker_id = worker_info.id\n",
        "\n",
        "            # iter_start and iter_end are now different for each worker. This avoids duplication of data across worker processes\n",
        "            iter_start = self.start + worker_id * per_worker\n",
        "            iter_end = min(iter_start + per_worker, self.end)\n",
        "\n",
        "        # return an iterable\n",
        "        return iter(range(iter_start, iter_end))"
      ],
      "metadata": {
        "id": "AOqwR-sA_ydF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map-style vs Iterable Datasets\n",
        "\n",
        "Until now, we learned about map style datasets and iterable style datasets individually while understanding how each works mechanically.\n",
        "\n",
        "\n",
        "However, making a brief yet clear distinction between the two is crucial to enhance understanding. Therefore, we lay below some points of difference that conceptually differentiate map-style datasets from iterable-style datasets.\n",
        "\n",
        "\n",
        "Map style datasets are useful when reading the data randomly through subsequent calls like dataset[idx] is viable, and the length of the full dataset is known beforehand. In contrast, iterable style datasets are used in cases when the data comes sequentially from a stream, and the full length of how large the dataset is could be unknown.\n",
        "\n",
        "\n",
        "With map-style datasets, a single item could be loaded at a time, like so:\n",
        "\n",
        "\n",
        "for the index in the sampler:\n",
        "    sample = dataset[index]\n",
        "\n",
        "\n",
        "On the other hand, with iterable style datasets, it is possible to load a batch (or a single item too) of data at a time, like so :\n",
        "\n",
        "\n",
        "for batch in iter(dataset):  # batch/sample depending on the implementation\n",
        "    body\n",
        "\n",
        "\n",
        "The data loading order for map-style datasets is controlled by the dataloader class using another Sampler class. Hence, the DataLoader samples indices of the items to be loaded (this is also where the len method implemented in custom dataset classes is used). On the other hand, the data loading order in iterable style datasets is completely determined by the user implementation, and the dataloader has nothing to do with it.\n",
        "\n",
        "\n",
        "In the case of map-style datasets, the DataLoader specifies a fixed batch size and takes care of collation (possibly with user-defined custom collate_fn). In contrast, with iterable style datasets, batching could be implemented in the iter protocol itself.\n",
        "\n",
        "\n",
        "We must be careful with multiprocessing when dealing with iterable style datasets as specified above (by using worker info). However, with map-style datasets, the dataloader internally applies to shard in every dataset copy across the different worker processes."
      ],
      "metadata": {
        "id": "zIxK-pZg_9S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_iter,test_iter = IMDB()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n"
      ],
      "metadata": {
        "id": "WY5KsX-wMZvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poUUmmWNMoXd",
        "outputId": "7ca8288a-3ae6-4a03-eaf0-b640c30d9573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,\n",
              " 'Along with \"King of the Rocket Men\", this was still being repeated on BBC TV in the early to mid eighties. If I was loading up a time capsule of this period both these series would definitely go in.<br /><br />Someone watching it for the first time will think it is silly but this is one of the best examples of the \"Serials\". Don Del Oro will make you laugh (When I was little my nickname for him was Mr Dustbin head) and it was funny upon being shot at he says \"Your bullets can\\'t harm me\" then he stumbles back, seemingly less than happy. I also like the way he dispenses with Sebastian in the first episode.<br /><br />I watched this again because I had good memories of it from years back, there are some good stunts and good music, it has the ingredients you expect including water,rockfalls,runaway carts... Apart from the first episode(with Ralph Faulkner)the swordplay wasn\\'t nearly as good as I remembered it, and yes it features the inevitable \"flashback\" episode! It gets 8 out of 10 because it still suffers from slow pace, padding and the other tricks. If you are interested in these serials I recommend the book by William Witney, \"In a Door, Into a Fight, Out a Door, Into a Chase\" although there is only a small entry about this series in it.')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data processing pipelines\n",
        "\n",
        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer. Those are the basic data processing building blocks for raw text string.\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
      ],
      "metadata": {
        "id": "Nosri1UK-rG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have to build a _vocabulary_. This is a effectively a look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment5.png?raw=1)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one).\n",
        "\n",
        "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\".\n",
        "\n",
        "The following builds the vocabulary, only keeping the most common `max_size` tokens."
      ],
      "metadata": {
        "id": "X0I-dlqWBYDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
        "\n",
        "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment6.png?raw=1)\n",
        "\n",
        "We can also view the most common words in the vocabulary and their frequencies."
      ],
      "metadata": {
        "id": "wWEwsUrIBgaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
        "                                  specials=[\"<unk>\", \"<pad>\"],\n",
        "                                  max_tokens= 25000)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "FKimuiq1N9l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "uBfAwMtKPHQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nYfpzXFIPT1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for i in train_iter:\n",
        "\n",
        "    print(i)\n",
        "    cnt +=1\n",
        "    if cnt == 1:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6aIoNksP5F4",
        "outputId": "ed35ad17-1b8f-4e91-dcec-7ec04a0f40ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate data batch and iterator\n",
        "\n",
        "torch.utils.data.DataLoader is recommended for PyTorch users (a tutorial is here). It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.\n",
        "\n",
        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker.\n",
        "\n",
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
      ],
      "metadata": {
        "id": "Sss_YtAg-vvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#collate_function: process the list of samples to form a batch.\n",
        "# https://androidkt.com/create-dataloader-with-collate_fn-for-variable-length-input-in-pytorch/\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    label_list, text_list= [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64).unsqueeze(dim=1)\n",
        "    text_list = pad_sequence(text_list, padding_value = 1)\n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "kckk_492P86h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=16,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "y0R4aYlMR1IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hFOamoNS6IeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate.\n",
        "\n",
        "There is a small amount of boilerplate code when creating models in PyTorch, note how our `RNN` class is a sub-class of `nn.Module` and the use of `super`.\n",
        "\n",
        "Within the `__init__` we define the _layers_ of the module. Our three layers are an _embedding_ layer, our RNN, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
        "\n",
        "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
        "\n",
        "The `forward` method is called when we feed examples into our model.\n",
        "\n",
        "Each batch, `text`, is a tensor of size _**[sentence length, batch size]**_. That is a batch of sentences, each having each word converted into a one-hot vector.\n",
        "\n",
        "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*.\n",
        "\n",
        "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size _**[sentence length, batch size, embedding dim]**_.\n",
        "\n",
        "`embedded` is then fed into the RNN. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "\n",
        "The RNN returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of size _**[1, batch size, hidden dim]**_. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this using the `assert` statement. Note the `squeeze` method, which is used to remove a dimension of size 1.\n",
        "\n",
        "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."
      ],
      "metadata": {
        "id": "eukZuH-8B7Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "metadata": {
        "id": "x03C-F0ESCyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "metadata": {
        "id": "Brx60NQ3SGvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zugQW6yHS1RC",
        "outputId": "7e993c1a-440a-4c5c-9434-a311c979ea13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,591,905 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
      ],
      "metadata": {
        "id": "39oBqOOhS2oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc * 100"
      ],
      "metadata": {
        "id": "fu-FY4hDX6z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train` function iterates over all examples, one batch at a time.\n",
        "\n",
        "`model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
        "\n",
        "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "We then feed the batch of sentences, `text`, into the model. Note, you do not need to do `model.forward(text)`, simply calling the model works. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
        "\n",
        "The loss and accuracy are then calculated using our predictions and the labels, `label`, with the loss being averaged over all examples in the batch.\n",
        "\n",
        "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
        "\n",
        "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value."
      ],
      "metadata": {
        "id": "y44upCohCK26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "a = []\n",
        "b = []\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text)\n",
        "        loss = criterion(predicted_label, label.float())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += binary_accuracy(predicted_label,label)\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/log_interval))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n"
      ],
      "metadata": {
        "id": "jCssf4LQS4nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "\n",
        "`model.eval()` puts the model in \"evaluation mode\", this turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
      ],
      "metadata": {
        "id": "DR5iEjqDCHnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model(text)\n",
        "            loss = criterion(predicted_label, label.float())\n",
        "            total_acc += binary_accuracy(predicted_label,label)\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/len(dataloader)"
      ],
      "metadata": {
        "id": "y7QQshWNCInH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "total_accu = None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgCeiruETFBD",
        "outputId": "263e3857-4a7b-4508-bc5c-65cd59a2e91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/ 1485 batches | accuracy   51.188\n",
            "| epoch   1 |   200/ 1485 batches | accuracy   48.375\n",
            "| epoch   1 |   300/ 1485 batches | accuracy   50.188\n",
            "| epoch   1 |   400/ 1485 batches | accuracy   48.750\n",
            "| epoch   1 |   500/ 1485 batches | accuracy   49.438\n",
            "| epoch   1 |   600/ 1485 batches | accuracy   49.188\n",
            "| epoch   1 |   700/ 1485 batches | accuracy   49.125\n",
            "| epoch   1 |   800/ 1485 batches | accuracy   50.500\n",
            "| epoch   1 |   900/ 1485 batches | accuracy   49.750\n",
            "| epoch   1 |  1000/ 1485 batches | accuracy   49.562\n",
            "| epoch   1 |  1100/ 1485 batches | accuracy   49.938\n",
            "| epoch   1 |  1200/ 1485 batches | accuracy   51.812\n",
            "| epoch   1 |  1300/ 1485 batches | accuracy   51.438\n",
            "| epoch   1 |  1400/ 1485 batches | accuracy   48.125\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 19.39s | valid accuracy   51.899 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   100/ 1485 batches | accuracy   51.000\n",
            "| epoch   2 |   200/ 1485 batches | accuracy   49.500\n",
            "| epoch   2 |   300/ 1485 batches | accuracy   49.812\n",
            "| epoch   2 |   400/ 1485 batches | accuracy   47.688\n",
            "| epoch   2 |   500/ 1485 batches | accuracy   50.875\n",
            "| epoch   2 |   600/ 1485 batches | accuracy   48.750\n",
            "| epoch   2 |   700/ 1485 batches | accuracy   50.000\n",
            "| epoch   2 |   800/ 1485 batches | accuracy   51.000\n",
            "| epoch   2 |   900/ 1485 batches | accuracy   49.625\n",
            "| epoch   2 |  1000/ 1485 batches | accuracy   49.062\n",
            "| epoch   2 |  1100/ 1485 batches | accuracy   49.938\n",
            "| epoch   2 |  1200/ 1485 batches | accuracy   50.312\n",
            "| epoch   2 |  1300/ 1485 batches | accuracy   52.062\n",
            "| epoch   2 |  1400/ 1485 batches | accuracy   48.688\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 18.29s | valid accuracy   52.532 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   100/ 1485 batches | accuracy   50.812\n",
            "| epoch   3 |   200/ 1485 batches | accuracy   50.688\n",
            "| epoch   3 |   300/ 1485 batches | accuracy   49.125\n",
            "| epoch   3 |   400/ 1485 batches | accuracy   49.375\n",
            "| epoch   3 |   500/ 1485 batches | accuracy   48.062\n",
            "| epoch   3 |   600/ 1485 batches | accuracy   49.812\n",
            "| epoch   3 |   700/ 1485 batches | accuracy   49.125\n",
            "| epoch   3 |   800/ 1485 batches | accuracy   49.938\n",
            "| epoch   3 |   900/ 1485 batches | accuracy   50.250\n",
            "| epoch   3 |  1000/ 1485 batches | accuracy   49.875\n",
            "| epoch   3 |  1100/ 1485 batches | accuracy   50.438\n",
            "| epoch   3 |  1200/ 1485 batches | accuracy   48.062\n",
            "| epoch   3 |  1300/ 1485 batches | accuracy   51.375\n",
            "| epoch   3 |  1400/ 1485 batches | accuracy   48.938\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 17.25s | valid accuracy   51.345 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   100/ 1485 batches | accuracy   50.062\n",
            "| epoch   4 |   200/ 1485 batches | accuracy   50.000\n",
            "| epoch   4 |   300/ 1485 batches | accuracy   48.812\n",
            "| epoch   4 |   400/ 1485 batches | accuracy   50.125\n",
            "| epoch   4 |   500/ 1485 batches | accuracy   51.188\n",
            "| epoch   4 |   600/ 1485 batches | accuracy   51.312\n",
            "| epoch   4 |   700/ 1485 batches | accuracy   47.375\n",
            "| epoch   4 |   800/ 1485 batches | accuracy   52.375\n",
            "| epoch   4 |   900/ 1485 batches | accuracy   50.312\n",
            "| epoch   4 |  1000/ 1485 batches | accuracy   49.625\n",
            "| epoch   4 |  1100/ 1485 batches | accuracy   50.375\n",
            "| epoch   4 |  1200/ 1485 batches | accuracy   50.062\n",
            "| epoch   4 |  1300/ 1485 batches | accuracy   49.312\n",
            "| epoch   4 |  1400/ 1485 batches | accuracy   50.562\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.38s | valid accuracy   50.475 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   100/ 1485 batches | accuracy   51.375\n",
            "| epoch   5 |   200/ 1485 batches | accuracy   50.812\n",
            "| epoch   5 |   300/ 1485 batches | accuracy   48.312\n",
            "| epoch   5 |   400/ 1485 batches | accuracy   48.625\n",
            "| epoch   5 |   500/ 1485 batches | accuracy   51.188\n",
            "| epoch   5 |   600/ 1485 batches | accuracy   51.938\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-15874b1be687>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0maccu_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0maccu_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-4ded4dd4e73d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqTV2QSWDGzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}