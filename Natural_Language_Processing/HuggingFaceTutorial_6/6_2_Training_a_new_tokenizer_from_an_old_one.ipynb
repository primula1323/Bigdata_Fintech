{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBcA0C1IAYAJ"
   },
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIu8kFnxAYAL"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXxvUobpAYAM"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece]\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He6KVN3uAYAN"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oK_B2OSiAYAN"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"hpshin@snu.ac.kr\"\n",
    "!git config --global user.name \"Hyopil Shin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a new Tokenizer from an old one\n",
    "\n",
    "**If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data.** That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in [Chapter 2](https://huggingface.co/course/chapter2), we saw that most Transformer models use a **subword** tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call **training**. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"430\" height=\"320\" src=\"https://www.youtube.com/embed/DJimQynXZsQ\" title=\"Training a new tokenizer\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch**. Itâ€™s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). **Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm**. Itâ€™s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling a corpus\n",
    "\n",
    "Thereâ€™s a very simple API in ðŸ¤— Transformers that you can use to train a new tokenizer with the same characteristics as an existing one: \n",
    "**AutoTokenizer.train_new_from_iterator()**. To see this in action, letâ€™s say we want to train GPT-2 from scratch, but in a language other than English. Our first task will be to gather lots of data in that language in a training corpus. To provide examples everyone will be able to understand, we wonâ€™t use a language like Russian or Chinese here, but rather a specialized English language: Python code.\n",
    "\n",
    "The ðŸ¤— [Datasets](https://github.com/huggingface/datasets) library can help us assemble a corpus of Python source code. Weâ€™ll use the usual load_dataset() function to download and cache the [CodeSearchNet](https://huggingface.co/datasets/code_search_net) dataset. This dataset was created for the [CodeSearchNet](https://wandb.ai/github/CodeSearchNet/benchmark) challenge and contains millions of functions from open source libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy5Xo0MxAYAN"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KC7s96txAYAO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34a153766db4d24932d5bcda6ecf923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nNOjlgquAYAO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34f6481675d4c0aaac2a9ad64744872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ff5eb59f484ce6803c36947cef0ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89580a717e4c47ec886bad0fbc4f4b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset code_search_net/python to /home/hpshin/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f98c454dc84cfd986a1163f08254cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e037921800a14c339869bc5a8977756b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d147e7c902f4ad9a68e4c14c3733ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b3bd8d894a456c92da4bc877c9950b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236817fa5998412589b45730f9cd9190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3873461dacd4e9e90570ad683272829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48dfcd79e4644c2867cd6e898b0d89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce58b85d4099435da3c087105d42cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76cb83a609d4345ac85ab57f402f0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset code_search_net downloaded and prepared to /home/hpshin/.cache/huggingface/datasets/code_search_net/python/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828d80109fa849608e4c09c90556c81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the training split to see which columns we have access to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZU6eSJ-rAYAO",
    "outputId": "bcda5329-2a6d-4d4c-ae73-56d7df08a052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the dataset separates docstrings from code and suggests a tokenization of both. Here. **weâ€™ll just use the whole_func_string column to train our tokenizer**. We can look at an example of one these functions by indexing into the train split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lvoTJoteAYAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def core_properties(self):\n",
      "        \"\"\"\n",
      "        Instance of |CoreProperties| holding the read/write Dublin Core\n",
      "        document properties for this presentation. Creates a default core\n",
      "        properties part if one is not present (not common).\n",
      "        \"\"\"\n",
      "        try:\n",
      "            return self.part_related_by(RT.CORE_PROPERTIES)\n",
      "        except KeyError:\n",
      "            core_props = CorePropertiesPart.default()\n",
      "            self.relate_to(core_props, RT.CORE_PROPERTIES)\n",
      "            return core_props\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is **transform the dataset into an iterator of lists of texts** â€” for instance, a list of list of texts. **Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once. If your corpus is huge, you will want to take advantage of the fact that ðŸ¤— Datasets does not load everything into RAM but stores the elements of the dataset on disk.**\n",
    "\n",
    "Doing the following would create a list of lists of 1,000 texts each, but would load everything in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY6-TLw_AYAP"
   },
   "outputs": [],
   "source": [
    "# Don't uncomment the following line unless your dataset is small!\n",
    "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a **Python generator, we can avoid Python loading anything into memory until itâ€™s actually necessary.** To create such a generator, you just to need to replace the brackets with **parentheses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C1IwGAX7AYAQ"
   },
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code doesnâ€™t fetch any elements of the dataset; **it just creates an object you can use in a Python for loop. The texts will only be loaded when you need them (that is, when youâ€™re at the step of the for loop that requires them), and only 1,000 texts at a time will be loaded. This way you wonâ€™t exhaust all your memory even if you are processing a huge dataset.**\n",
    "\n",
    "**The problem with a generator object is that it can only be used once.** So, instead of this giving us the list of the first 10 digits twice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get them once and then an empty list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vS5uUWpBAYAQ",
    "outputId": "13b5316e-486e-46f2-c80b-5663c4a8a16d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thatâ€™s why we define a function that returns a generator instead:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5DBoMN9NAYAQ"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define your generator inside a for loop by using the yield statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mm8JNmc5AYAQ"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will produce the exact same generator as before, but allows you to use more complex logic than you can in a list comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a new tokenizer\n",
    "\n",
    "Now that we have our corpus in the form of an iterator of batches of texts, we are ready to train a new tokenizer. To do this, we first need to load the tokenizer we want to pair with our model (here, **GPT-2**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "loejEDflAYAR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:45:28.680109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 16:45:28.814556: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-18 16:45:28.844658: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-18 16:45:29.310102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib64::/usr/local/lib64:\n",
      "2023-05-18 16:45:29.310188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib64::/usr/local/lib64:\n",
      "2023-05-18 16:45:29.310195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, **itâ€™s a good idea to do this to avoid starting entirely from scratch. This way, we wonâ€™t have to specify anything about the tokenization algorithm or the special tokens we want to use;** our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the **vocabulary**, which will be determined by the training on our corpus.\n",
    "\n",
    "First letâ€™s have a look at how this tokenizer would treat an example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wWxXU7XrAYAR",
    "outputId": "7e41f900-93ca-423c-b9ab-6e86fd90ef60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ä add',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ä b',\n",
       " '):',\n",
       " 'ÄŠ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä \"\"\"',\n",
       " 'Add',\n",
       " 'Ä the',\n",
       " 'Ä two',\n",
       " 'Ä numbers',\n",
       " 'Ä `',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ä and',\n",
       " 'Ä `',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'ÄŠ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä ',\n",
       " 'Ä return',\n",
       " 'Ä a',\n",
       " 'Ä +',\n",
       " 'Ä b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer has a few special symbols, like *Ä * and *ÄŠ*, which denote **spaces** and **newlines**, respectively. As we can see, **this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the _ character.**\n",
    "\n",
    "Letâ€™s train a new tokenizer and see if it solves those issues. For this, weâ€™ll use the method **train_new_from_iterator():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_a2XWos6AYAR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command might take a bit of time if your corpus is very large, but for this dataset of 1.6 GB of texts itâ€™s blazing fast (1 minute 16 seconds on an AMD Ryzen 9 3900X CPU with 12 cores).\n",
    "\n",
    "Note that **AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a â€œfastâ€ tokenizer.** As youâ€™ll see in the next section, the ðŸ¤— Transformers library contains two types of tokenizers: some are written purely in Python and others (the fast ones) are backed by the ðŸ¤— Tokenizers library, which is written in the [Rust](https://www.rust-lang.org/) programming language. Python is the language most often used for data science and deep learning applications, but when anything needs to be parallelized to be fast, it has to be written in another language. For instance, the matrix multiplications that are at the core of the model computation are written in CUDA, an optimized C library for GPUs.\n",
    "\n",
    "**Training a brand new tokenizer in pure Python would be excruciatingly slow, which is why we developed the ðŸ¤— Tokenizers library. Note that just as you didnâ€™t have to learn the CUDA language to be able to execute your model on a batch of inputs on a GPU, you wonâ€™t need to learn Rust to use a fast tokenizer. The ðŸ¤— **Tokenizers library provides Python bindings for many methods that internally call some piece of code in Rust;** for example, to parallelize the training of your new tokenizer or, as we saw in [Chapter 3](https://huggingface.co/course/chapter3), the tokenization of a batch of inputs.\n",
    "\n",
    "**Most of the Transformer models have a fast tokenizer available (there are some exceptions that you can check here), and the AutoTokenizer API always selects the fast tokenizer for you if itâ€™s available.** In the next section weâ€™ll take a look at some of the other special features fast tokenizers have, which will be really useful for tasks like token classification and question answering. Before diving into that, however, letâ€™s try our brand new tokenizer on the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-uMmysDaAYAR",
    "outputId": "d3a083ac-7299-48df-974a-05db28df3b59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ä add',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ä b',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä \"\"\"',\n",
       " 'Add',\n",
       " 'Ä the',\n",
       " 'Ä two',\n",
       " 'Ä numbers',\n",
       " 'Ä `',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ä and',\n",
       " 'Ä `',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä return',\n",
       " 'Ä a',\n",
       " 'Ä +',\n",
       " 'Ä b']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again see the special symbols Ä  and ÄŠ that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of **Python functions: for example, there is a ÄŠÄ Ä Ä  token that represents an indentation, and a Ä \"\"\" token that represents the three quotes that start a docstring. The tokenizer also correctly split the function name on _.** This is quite a compact representation; comparatively, using the plain English tokenizer on the same example will give us a longer sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8sRNo_xbAYAS",
    "outputId": "72044bd9-55ad-496b-cb7c-85cf9ae08e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xTENz4u6AYAS",
    "outputId": "3afc1cf3-3a56-41d2-d092-1cc2f6c3994b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'Ä Linear',\n",
       " 'Layer',\n",
       " '():',\n",
       " 'ÄŠÄ Ä Ä ',\n",
       " 'Ä def',\n",
       " 'Ä __',\n",
       " 'init',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ä input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ä output',\n",
       " '_',\n",
       " 'size',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'weight',\n",
       " 'Ä =',\n",
       " 'Ä torch',\n",
       " '.',\n",
       " 'randn',\n",
       " '(',\n",
       " 'input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ä output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'Ä =',\n",
       " 'Ä torch',\n",
       " '.',\n",
       " 'zeros',\n",
       " '(',\n",
       " 'output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ÄŠÄŠÄ Ä Ä ',\n",
       " 'Ä def',\n",
       " 'Ä __',\n",
       " 'call',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ä x',\n",
       " '):',\n",
       " 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
       " 'Ä return',\n",
       " 'Ä x',\n",
       " 'Ä @',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'weights',\n",
       " 'Ä +',\n",
       " 'Ä self',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ÄŠÄ Ä Ä Ä ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: ÄŠÄ Ä Ä Ä Ä Ä Ä . The special Python words like class, init, call, self, and return are each tokenized as one token, and we can see that as well as splitting on _ and . the tokenizer correctly splits even camel-cased names: LinearLayer is tokenized as [\"Ä Linear\", \"Layer\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the tokenizer\n",
    "\n",
    "To make sure we can use it later, we need to save our new tokenizer. Like for models, this is done with the save_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6Ba12b-EAYAS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create **a new folder named *code-search-net-tokenizer*, which will contain all the files the tokenizer needs to be reloaded.** If you want to share this tokenizer with your colleagues and friends, you can upload it to the Hub by logging into your account. If youâ€™re working in a notebook, thereâ€™s a convenience function to help you with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9MwLE8AkAYAS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26074c29ba17496f8a299cf525758dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display a widget where you can enter your Hugging Face login credentials. If you arenâ€™t working in a notebook, just type the following line in your terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once youâ€™ve logged in, you can push your tokenizer by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bBg0RcsOAYAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If not specifying `clone_from`, you need to pass Repository a valid git clone.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode-search-net-tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.9/site-packages/transformers/file_utils.py:2172\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_path_or_name, repo_url, use_temp_dir, commit_message, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m   2169\u001b[0m     repo_path_or_name \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp()\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;66;03m# Create or clone the repo. If the repo is already cloned, this just retrieves the path to the repo.\u001b[39;00m\n\u001b[0;32m-> 2172\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_or_get_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_path_or_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_path_or_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;66;03m# Save the files in the cloned repo\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_pretrained(repo_path_or_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.9/site-packages/transformers/file_utils.py:2248\u001b[0m, in \u001b[0;36mPushToHubMixin._create_or_get_repo\u001b[0;34m(cls, repo_path_or_name, repo_url, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(repo_path_or_name):\n\u001b[1;32m   2246\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(repo_path_or_name)\n\u001b[0;32m-> 2248\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_path_or_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2249\u001b[0m repo\u001b[38;5;241m.\u001b[39mgit_pull()\n\u001b[1;32m   2250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repo\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:98\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m     97\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.9/site-packages/huggingface_hub/repository.py:512\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    510\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Repository] is a valid git repo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf not specifying `clone_from`, you need to pass Repository a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m valid git clone.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    518\u001b[0m     git_email \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m git_user \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ):\n\u001b[1;32m    520\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwhoami(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token)\n",
      "\u001b[0;31mValueError\u001b[0m: If not specifying `clone_from`, you need to pass Repository a valid git clone."
     ]
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a new repository in your namespace with the name code-search-net-tokenizer, containing the tokenizer file. You can then load the tokenizer from anywhere with the from_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVvHUWqtAYAS"
   },
   "outputs": [],
   "source": [
    "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™re now all set for training a language model from scratch and fine-tuning it on your task at hand! Weâ€™ll get to that in [Chapter 7](https://huggingface.co/course/chapter7), but first, in the rest of this chapter weâ€™ll take a closer look at fast tokenizers and explore in detail what actually happens when we call the method train_new_from_iterator()."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
