{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from typing import Union, List\n",
    "#import ISLP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor0420:\n",
    "    def __init__(self):\n",
    "        self.file_path = \"\"\n",
    "        self.folder_path = \"\"\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def __init__(self, data_file_path:str=\"\", folder_path:str=\"\"):\n",
    "        self.file_path = data_file_path\n",
    "        self.folder_path = folder_path\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def load_origin_file(self, file_path:str)->None:\n",
    "        if file_path!=\"\":\n",
    "            self.file_path = file_path\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "\n",
    "    def load_modified_file(self, file_path:str)->None:\n",
    "        if file_path!=\"\":\n",
    "            self.file_path = file_path\n",
    "        self.df = pd.read_csv(self.file_path)\n",
    "        if 'Unnamed: 0' in self.df.columns:\n",
    "            self.df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    def drop_columns(self, drop_columns_file_path:str)->None:\n",
    "        return None\n",
    "\n",
    "    def __preprocess_target_variable(self, target_variable:str=\"loan_status\")->None:\n",
    "        return None\n",
    "\n",
    "    def __fill_na_with_value(self, columns:List[str], filling_value:Union[str, int])->None:\n",
    "        '''\n",
    "        df: dataframe to fill NA\n",
    "        column_name : column name to change NA values\n",
    "        filling_value : value type or just value to fill column's NA\n",
    "        '''\n",
    "        # for column_name in columns:\n",
    "        #     if filling_value==\"mode\":\n",
    "        #         mode_value = self.df[column_name].mode()[0]\n",
    "        #     elif filling_value==\"median\":\n",
    "        #         mode_value = self.df[column_name].median()\n",
    "        #     else:\n",
    "        #         mode_value = filling_value\n",
    "        #     self.df[column_name].fillna(mode_value, inplace=True)\n",
    "        return None\n",
    "\n",
    "    def __preprocessing_na(self)->None:\n",
    "        ## 결측 처리\n",
    "        # 결측 개수가 1천 건 이하인 경우는 해당 데이터(row) 삭제\n",
    "        # A1. 최빈값 대체\n",
    "        # A2. 중앙값 대체\n",
    "        # B. 2015년 대체\n",
    "        # C. 2012년 대체\n",
    "        # D. 결측 0 대체\n",
    "        return None\n",
    "\n",
    "    def __Multicollinearity(self)->None:\n",
    "        # self.df.drop(columns=['fico_range_low'], inplace=True)\n",
    "        self.df['fico_avg'] = (self.df['fico_range_low'] + self.df['fico_range_high'])/2\n",
    "        self.df.drop(columns=['fico_range_low', 'fico_range_high'], inplace=True)\n",
    "\n",
    "    def __convert_log(self, term):\n",
    "        return np.log(term + 1)\n",
    "\n",
    "    def __log_transform(self):\n",
    "        variables = [\n",
    "          \"all_util\", \"annual_inc\", \"annual_inc_joint\", \"bc_open_to_buy\", # \"avg_cur_bal\",\n",
    "          \"delinq_amnt\", \"dti\", \"max_bal_bc\", \"mo_sin_old_il_acct\", \"mo_sin_old_rev_tl_op\",\n",
    "          \"mo_sin_rcnt_rev_tl_op\", \"mo_sin_rcnt_tl\", \"mort_acc\", \"mths_since_rcnt_il\",\n",
    "          \"mths_since_recent_bc\", \"num_accts_ever_120_pd\", \"num_actv_bc_tl\", \"num_actv_rev_tl\",\n",
    "          \"num_bc_sats\", \"num_bc_tl\", \"num_il_tl\", \"num_op_rev_tl\", \"num_rev_accts\",\n",
    "          \"num_rev_tl_bal_gt_0\", \"num_sats\", \"open_acc\", \"open_acc_6m\", \"open_act_il\",\n",
    "          \"open_il_12m\", \"open_il_24m\", \"open_rv_12m\", \"open_rv_24m\", \"pub_rec_bankruptcies\",\n",
    "          \"revol_bal\", \"revol_bal_joint\", \"tax_liens\", \"tot_cur_bal\", \"tot_hi_cred_lim\",\n",
    "          \"total_acc\", \"total_bal_ex_mort\", \"total_bal_il\", \"total_bc_limit\", \"total_cu_tl\",\n",
    "          \"total_il_high_credit_limit\", \"total_rev_hi_lim\"\n",
    "        ]\n",
    "\n",
    "        # 각 변수에 대해 로그 변환 수행 및 기존 변수 삭제\n",
    "        for var in variables:\n",
    "            # 로그 변환 후 변수 이름에 '_log'를 추가하여 새로운 변수 생성\n",
    "            new_var = var + \"_log\"\n",
    "            # 해당 변수가 0보다 큰 경우에만 로그 변환 수행하여 음수 무한대를 방지\n",
    "            # 로그 변환 후에는 기존 값이 0인 경우 음수 무한대로 처리되므로 이에 대한 처리도 필요\n",
    "            # 여기서 로그는 자연로그 (밑이 e인 로그)\n",
    "            self.df[new_var] = np.log(self.df[var] + 1)  # 0이 아닌 값이어야 하므로 +1 추가\n",
    "            # 기존 변수 삭제\n",
    "            self.df.drop(columns=[var], inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def __convert_object_to_numeric(self, column_name:str)->pd.DataFrame:\n",
    "        return None\n",
    "\n",
    "    def __convert_object_to_one_hot(self, column_name:str)->None:\n",
    "        return None\n",
    "\n",
    "    def __preprocessing_objects(self)->None:\n",
    "        return None\n",
    "\n",
    "    def preprocess(self)->None:\n",
    "        # loan_status 제외 모든 column이 결측치(na)인 행 제거 (1개 행 제거됨)\n",
    "        self.df.dropna(subset=self.df.columns.difference(['loan_status']),how='all', inplace=True)\n",
    "        self.__preprocess_target_variable()\n",
    "        # 결측치 제거\n",
    "        self.__preprocessing_na()\n",
    "        ## object 처리하기\n",
    "        self.__preprocessing_objects()\n",
    "        ## 다중공선성 제거 - 0419 추가\n",
    "        self.__Multicollinearity()\n",
    "        ## 로그변환 - 0419 추가\n",
    "        self.__log_transform()\n",
    "        # index 재설정\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "        self.df.dropna(subset=self.df.columns.difference(['loan_status']),how='all', inplace=True)\n",
    "\n",
    "    def get_df(self)->pd.DataFrame:\n",
    "        return self.df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocessor0420()\n",
    "# lending_club_2020_train.csv 파일이 있는 절대 경로 혹은 상대 경로를 명시해주세요\n",
    "p.load_modified_file(file_path=\"modified_0419.csv\")\n",
    "# preprocess를 돌리기\n",
    "p.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1131682, 90)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = p.get_df()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(columns=['loan_status'])\n",
    "df_y = df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "clf = RidgeClassifierCV().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89    180516\n",
      "           1       0.59      0.03      0.06     45821\n",
      "\n",
      "    accuracy                           0.80    226337\n",
      "   macro avg       0.70      0.51      0.47    226337\n",
      "weighted avg       0.76      0.80      0.72    226337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LassoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(penalty='l1', solver = 'saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89    180516\n",
      "           1       0.50      0.05      0.09     45821\n",
      "\n",
      "    accuracy                           0.80    226337\n",
      "   macro avg       0.65      0.52      0.49    226337\n",
      "weighted avg       0.74      0.80      0.72    226337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "#48분 걸려서 recall 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA, QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89    180516\n",
      "           1       0.51      0.10      0.16     45821\n",
      "\n",
      "    accuracy                           0.80    226337\n",
      "   macro avg       0.66      0.54      0.52    226337\n",
      "weighted avg       0.75      0.80      0.74    226337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.70      0.77    180516\n",
      "           1       0.30      0.52      0.38     45821\n",
      "\n",
      "    accuracy                           0.66    226337\n",
      "   macro avg       0.58      0.61      0.57    226337\n",
      "weighted avg       0.74      0.66      0.69    226337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "clf = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82    180516\n",
      "           1       0.35      0.41      0.37     45821\n",
      "\n",
      "    accuracy                           0.73    226337\n",
      "   macro avg       0.59      0.61      0.60    226337\n",
      "weighted avg       0.74      0.73      0.73    226337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
