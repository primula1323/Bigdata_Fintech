{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\AppData\\Local\\Temp\\ipykernel_4040\\2470395882.py:14: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband                 # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                                     # type: ignore\n",
    "import numpy as np                                      # type: ignore\n",
    "import warnings                                         # type: ignore\n",
    "from sklearn.model_selection import train_test_split    # type: ignore\n",
    "from sklearn.decomposition import PCA                   # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler# type: ignore\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # type: ignore\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve, make_scorer# type: ignore\n",
    "import tensorflow as tf                                 # type: ignore\n",
    "from tensorflow import keras                            # type: ignore\n",
    "from keras.layers import Dense, Dropout                 # type: ignore\n",
    "from keras.optimizers import Adam, SGD                  # type: ignore\n",
    "from keras.callbacks import EarlyStopping               # type: ignore\n",
    "from tensorboard.plugins.hparams import api as hp       # type: ignore\n",
    "import matplotlib.pyplot as plt                         # type: ignore\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE# type: ignore\n",
    "from imblearn.under_sampling import TomekLinks          # type: ignore\n",
    "from itertools import combinations_with_replacement\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(physical_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_RECOVERY_RATE = 0.44\n",
    "RISK_FREE = 1.04\n",
    "origin_train_df = pd.read_csv(\"train.csv\")\n",
    "origin_test_df = pd.read_csv(\"test.csv\")\n",
    "def get_expected_train_roe(y_pred, y_true):\n",
    "    earning = np.sum(origin_train_df.loc[np.where((y_pred==0)&(y_true==0))]['total_loan_amnt'])\n",
    "    recevored = np.sum(origin_train_df.loc[np.where((y_pred==0)&(y_true==1))]['total_loan_amnt'])*WEIGHTED_RECOVERY_RATE\n",
    "    risk_free = np.sum(origin_train_df.loc[np.where(y_pred==1)]['total_loan_amnt'])*RISK_FREE\n",
    "    return earning+recevored+risk_free\n",
    "\n",
    "def get_expected_train_roe(y_pred, y_true):\n",
    "    earning = np.sum(origin_test_df.loc[np.where((y_pred==0)&(y_true==0))]['total_loan_amnt'])\n",
    "    recevored = np.sum(origin_test_df.loc[np.where((y_pred==0)&(y_true==1))]['total_loan_amnt'])*WEIGHTED_RECOVERY_RATE\n",
    "    risk_free = np.sum(origin_test_df.loc[np.where(y_pred==1)]['total_loan_amnt'])*RISK_FREE\n",
    "    return earning+recevored+risk_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'total_loan_amnt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\tf_python_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'total_loan_amnt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43morigin_train_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_loan_amnt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\tf_python_310\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\tf_python_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'total_loan_amnt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 30\n",
    "\n",
    "class DNNModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = None\n",
    "        self.df = None\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.input_dim = 0\n",
    "        self.model_name = \"\"\n",
    "\n",
    "    def load_file(self, file_path:str)->None:\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        x_train = self.df.drop(columns=['loan_status', 'Unnamed: 0'])\n",
    "        # x_train = self.df.drop(columns=['loan_status'])\n",
    "        \n",
    "        y_train = self.df['loan_status']\n",
    "        nan_indices = np.isnan(x_train).any(axis=1)\n",
    "        self.x_train = x_train[~nan_indices]\n",
    "        self.y_train = y_train[~nan_indices]\n",
    "    \n",
    "    def __get_n_components_from_pca(self, scaled_data:pd.DataFrame, threshold:float) -> int:\n",
    "        pca = PCA()\n",
    "        pca.fit(scaled_data)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "        n_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "        return n_components\n",
    "    \n",
    "    def scaling_data(\n",
    "            self, \n",
    "            scaler:MinMaxScaler|StandardScaler=StandardScaler, \n",
    "            threshold:float=0.95, \n",
    "            over_sampler:SMOTE|RandomOverSampler=None, \n",
    "            under_sampler:TomekLinks=None)->None:\n",
    "        x_scaled = scaler().fit_transform(self.x_train)\n",
    "        self.input_dim = self.__get_n_components_from_pca(x_scaled, threshold=threshold)\n",
    "        self.model_name+=(f'scaler({scaler.__name__})_pca({(int(threshold*100))})_input({str(self.input_dim)})')\n",
    "        self.x_train = PCA(n_components=self.input_dim).fit_transform(x_scaled).astype('float32')\n",
    "        if over_sampler!=None:\n",
    "            os = over_sampler(random_state=RANDOM_STATE)\n",
    "            self.x_train, self.y_train = os.fit_resample(self.x_train,self.y_train)\n",
    "            self.model_name+=f'_sampler({os.__class__.__name__})'\n",
    "            print(f\"====Data set resampled(oversampled)_{os.__class__.__name__}\")\n",
    "        elif under_sampler!=None:\n",
    "            us = under_sampler(random_state=RANDOM_STATE, sampling_strategy='majority')\n",
    "            self.x_train, self.y_train = us.fit_resample(self.x_train, self.y_train)\n",
    "            self.model_name+=f'_sampler({us.__class__.__name__})'\n",
    "            print(f\"====Data set resampled(undersampled)_{us.__class__.__name__}\")\n",
    "        print(f\"==label ratio\")\n",
    "        print(f\"True Label:\\t{np.sum(self.y_train==1)/len(self.y_train)}\")\n",
    "        print(f\"False Label:\\t{np.sum(self.y_train==0)/len(self.y_train)}\")\n",
    "        self.x_train, self.x_validation, self.y_train, self.y_validation = train_test_split(self.x_train, self.y_train, test_size=0.2, stratify=self.y_train, random_state=RANDOM_STATE)\n",
    "    \n",
    "    def split_from_data(self):\n",
    "        self.x_train, self.x_validation, self.y_train, self.y_validation = train_test_split(self.x_train, self.y_train, test_size=0.2, stratify=self.y_train, random_state=RANDOM_STATE)\n",
    "\n",
    "    def __build_model(self, num_layers:int, num_nodes:int, loss:str, num_nodes_per_each_layers:list[int]=None)->keras.Sequential:\n",
    "        # Define and compile the model\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(num_nodes, input_dim=self.input_dim, activation='relu'))\n",
    "        for layer_index in range(num_layers):\n",
    "            if num_nodes_per_each_layers:\n",
    "                model.add(Dense(num_nodes_per_each_layers[layer_index], activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(num_nodes, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='Adam', loss=loss, metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    def searching_best_param_grid(\n",
    "            self, \n",
    "            grid_params:dict,\n",
    "            params:dict={'min_delta':0.001, \"n_splits\":5},\n",
    "            scoring='accuracy'\n",
    "            ):\n",
    "        kfold = StratifiedKFold(random_state=RANDOM_STATE,\n",
    "                n_splits=params['n_splits']\n",
    "                )\n",
    "        model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=self.__build_model)\n",
    "        # GridSearchCV 생성\n",
    "        grid_search = GridSearchCV(estimator = model,\n",
    "                                param_grid = grid_params,\n",
    "                                cv = kfold,\n",
    "                                scoring=scoring)\n",
    "        early_stopping = EarlyStopping(monitor='loss',min_delta=params['min_delta'])\n",
    "        # GridSearchCV fit 시작\n",
    "        grid_search.fit(self.x_train, self.y_train, callbacks=[early_stopping])\n",
    "        # 최적의 param\n",
    "        print(f\"Best params: {grid_search.best_params_}\")\n",
    "        # 최적의 param일 경우 최적의 accuracy\n",
    "        print(f\"Best average accuracy: {grid_search.best_score_}\")\n",
    "        return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "    def build_model_with_best_params(\n",
    "            self, \n",
    "            best_params:dict, \n",
    "            params:dict={'min_delta':0.001, \"n_splits\":5}, \n",
    "            threshold:float=0.5)->dict:\n",
    "        valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "        result_dict = {\n",
    "                       'average validation accuracy':0,\n",
    "                       'average validation recall':0,\n",
    "                       'average validation precision':0,\n",
    "                       'average validation f1':0,\n",
    "                       'validation accuracy':0,\n",
    "                       'validation recall':0,\n",
    "                       'validation precision':0,\n",
    "                       'validation f1':0,\n",
    "                       }\n",
    "        y_train_reshaped = np.reshape(self.y_train,(-1))\n",
    "        model = self.__build_model(best_params['num_layers'], best_params['num_nodes'], best_params['loss'])\n",
    "        kf = StratifiedKFold(n_splits=params['n_splits'], shuffle=True, random_state=RANDOM_STATE)\n",
    "        early_stopping = EarlyStopping(monitor='loss',min_delta=params['min_delta'])\n",
    "        for train_index, val_index in kf.split(self.x_train, y_train_reshaped):\n",
    "            X_train_fold, X_val_fold = self.x_train[train_index], self.x_train[val_index]\n",
    "            Y_train_fold, Y_val_fold = y_train_reshaped[train_index], y_train_reshaped[val_index]\n",
    "            # 모델 학습\n",
    "            model.fit(X_train_fold, \n",
    "                    Y_train_fold, \n",
    "                    batch_size=best_params['batch_size'], \n",
    "                    epochs=best_params['epochs'], \n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])\n",
    "            # 모델 validation\n",
    "            valid_loss, valid_acc= model.evaluate(X_val_fold, Y_val_fold)\n",
    "            pred = model.predict(X_val_fold).flatten()\n",
    "            pred = np.where(pred >= threshold, 1 , 0)\n",
    "            valid_accs.append(valid_acc)\n",
    "            valid_f1s.append(f1_score(Y_val_fold, pred))\n",
    "            valid_precisions.append(precision_score(Y_val_fold, pred))\n",
    "            valid_recalls.append(recall_score(Y_val_fold, pred))\n",
    "        result_dict['average validation accuracy'] = np.mean(valid_accs)\n",
    "        result_dict['average validation recall'] = np.mean(valid_recalls)\n",
    "        result_dict['average validation precision'] = np.mean(valid_precisions)\n",
    "        result_dict['average validation f1'] = np.mean(valid_f1s)\n",
    "        print(\"###################################\")\n",
    "        print(\"avg Validation accuracy:\", result_dict['average validation accuracy'])\n",
    "        print(\"avg Validation recall:\", result_dict['average validation recall'])\n",
    "        print(\"avg Validation precision:\", result_dict['average validation precision'])\n",
    "        print(\"avg Validation F1-score:\", result_dict['average validation f1'])\n",
    "\n",
    "        valid_pred = model.predict(self.x_validation).flatten()\n",
    "        valid_pred = np.where(valid_pred>=threshold, 1, 0)\n",
    "\n",
    "        result_dict['validation accuracy'] = accuracy_score(Y_val_fold, pred)\n",
    "        result_dict['validation f1'] = (f1_score(Y_val_fold, pred))\n",
    "        result_dict['validation precision'] = (precision_score(Y_val_fold, pred))\n",
    "        result_dict['validation recall'] = (recall_score(Y_val_fold, pred))\n",
    "        print(\"###################################\")\n",
    "        print(\"Validation accuracy:\", result_dict['validation accuracy'])\n",
    "        print(\"Validation recall:\", result_dict['validation recall'])\n",
    "        print(\"Validation precision:\", result_dict['validation precision'])\n",
    "        print(\"Validation F1-score:\", result_dict['validation f1'])\n",
    "        self.model = model\n",
    "        return result_dict\n",
    "    \n",
    "    def save_model(self, file_path:str=\"dnn_models/0425/dnn_\")->None:\n",
    "        self.model.save(file_path+self.model_name+'.h5')\n",
    " \n",
    "    def load_model(self, model_file_path:str=\"dnn_models/0425/dnn_\"):\n",
    "        self.model = tf.keras.models.load_model(model_file_path)\n",
    "\n",
    "    def get_input_dim(self)->int:\n",
    "        return self.input_dim\n",
    "\n",
    "    def get_model(self)->keras.Sequential:\n",
    "        return self.model\n",
    "\n",
    "    def get_model_name(self)->str:\n",
    "        return self.model_name\n",
    "\n",
    "    def get_roc_auc_score(self, save:bool=True)->float:\n",
    "        valid_pred = self.model.predict(self.x_validation).flatten()\n",
    "        fprs, tprs, thresholds = roc_curve(self.y_validation, valid_pred)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(fprs, tprs, label='ROC')\n",
    "        plt.xlabel(\"FPR(Fall-Out)\")\n",
    "        plt.ylabel(\"TPR(Recall):재현률\")\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(f'plots/{self.model_name}.png')\n",
    "        plt.show()\n",
    "        return roc_auc_score(self.y_validation, valid_pred)\n",
    "    \n",
    "    def draw_precision_recall_curve_plot(self, min_delta:float=0.01, save:bool=True)->float:\n",
    "        valid_pred = self.model.predict(self.x_validation).flatten()\n",
    "        precisions, recalls, thresholds = precision_recall_curve(self.y_validation, valid_pred)\n",
    "        # y축인 precisions와 recalls의 갯수 = threshold 갯수 + 1 이어서 x,y 갯수를 동일하게 맞춰줌\n",
    "        thres_boundary = thresholds.shape[0]\n",
    "        plt.plot(thresholds, precisions[:thres_boundary], \"r\", label=\"precision\")\n",
    "        plt.plot(thresholds, recalls[:thres_boundary], label=\"recall\")\n",
    "\n",
    "        # x축 스케일 0.1 단위로 조정\n",
    "        start, end = plt.xlim()\n",
    "        plt.xticks( np.round( np.arange(start, end, 0.1), 2))\n",
    "        # 라벨링\n",
    "        plt.xlabel(\"Threshold value\"), plt.ylabel(\"Precision and Recall value\")\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(f'plots/{self.model_name}_precision&recall.png')\n",
    "        plt.show()\n",
    "        min_delta_index = None\n",
    "        for i in range(len(precisions)):\n",
    "            delta = abs(precisions[i] - recalls[i])\n",
    "            if delta <= min_delta:\n",
    "                min_delta_index = i\n",
    "                break\n",
    "        return recalls[min_delta_index]\n",
    "    \n",
    "    def predict_test(self, test_x, test_y, threshold):\n",
    "        result_dict ={}\n",
    "        y_pred = self.model.predict(test_x).flatten()\n",
    "        y_pred = np.where(y_pred>=threshold, 1, 0)\n",
    "        print(\"###################################\")\n",
    "        print(\"Validation accuracy:\", accuracy_score(test_y, y_pred))\n",
    "        print(\"Validation recall:\", recall_score(test_y, y_pred))\n",
    "        print(\"Validation precision:\", precision_score(test_y, y_pred))\n",
    "        print(\"Validation F1-score:\", f1_score(test_y, y_pred))\n",
    "        return y_pred\n",
    "    \n",
    "    def get_y_train(self)->np.ndarray:\n",
    "        return self.y_train\n",
    "    \n",
    "    def get_y_validation(self)->np.ndarray:\n",
    "        return self.y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/modified_0420.csv\")\n",
    "train_df.drop(columns=['loan_status', 'Unnamed: 0'], inplace=True)\n",
    "sc, pca = get_scaler_pca_with_origin_data(StandardScaler, n_comp=86, origin_df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 86)                7482      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 86)                7482      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 86)                7482      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 86)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 86)                7482      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 86)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 87        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,015\n",
      "Trainable params: 30,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'dnn_scaler(StandardScaler)_pca(100)_input(86)_sampler(SMOTE).h5'\n",
    "dnn_model = DNNModel()\n",
    "test_df =pd.read_csv('data/modified_test.csv')\n",
    "dnn_model.load_model(model_file_path='dnn_models/0425/'+model_file_name)\n",
    "n_comp = dnn_model.get_model().summary()\n",
    "df =pd.read_csv('data/modified_0420.csv')\n",
    "X = df.drop(columns=['Unnamed: 0', 'loan_status'])\n",
    "Y = df['loan_status']\n",
    "resampled_x, resampled_y = SMOTE(random_state=RANDOM_STATE).fit_resample(X, Y)\n",
    "train_df = df.drop(columns=['loan_status', 'Unnamed: 0'])\n",
    "sc, pca = get_scaler_pca_with_origin_data(StandardScaler, n_comp=86, origin_df=resampled_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_df.drop(columns=['loan_status', 'Unnamed: 0']).to_numpy()\n",
    "test_y = test_df['loan_status'].to_numpy()\n",
    "test_x = sc.transform(test_x)\n",
    "test_x = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 18s 773us/step\n",
      "###################################\n",
      "Validation accuracy: 0.4782212328404092\n",
      "Validation recall: 0.8480439087725741\n",
      "Validation precision: 0.25868725096413886\n",
      "Validation F1-score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.predict_test(test_x, test_y, threshold=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
